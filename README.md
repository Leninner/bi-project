# ğŸ“Š Sistema de AnÃ¡lisis de Pobreza - BI Project

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto implementa un sistema completo de Business Intelligence (BI) para el anÃ¡lisis de pobreza, incluyendo:

- **Base de Datos MySQL**: Almacenamiento estructurado de datos demogrÃ¡ficos y laborales
- **Pipeline de Datos**: ExtracciÃ³n, transformaciÃ³n y preparaciÃ³n de datos para ML
- **AnÃ¡lisis Predictivo**: PreparaciÃ³n de datasets para modelos de machine learning

## ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TB
    A[MySQL Database] --> B[Data Pipeline]
    B --> C[CSV Files]
    C --> D[ML Ready Datasets]
    
    subgraph "MySQL Database"
        A1[fact_ocupacion]
        A2[dim_persona]
        A3[dim_tiempo]
        A4[dim_ciudad]
        A5[dim_sectorlaboral]
        A6[dim_condicionactividad]
    end
    
    subgraph "Data Pipeline"
        B1[Extract Data]
        B2[Transform Data]
        B3[Split Datasets]
        B4[Save to CSV]
    end
    
    subgraph "Output Files"
        C1[poverty_dataset.csv]
        C2[train_data.csv]
        C3[val_data.csv]
        C4[test_data.csv]
    end
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Python 3.8+
- MySQL Server
- Docker (opcional, para base de datos)

### 1. Clonar el Repositorio

```bash
git clone <repository-url>
cd bi/project
```

### 2. Configurar Entorno Virtual

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En Linux/Mac:
source venv/bin/activate
# En Windows:
venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Base de Datos MySQL

Este paso va a tomar el archivo `bi.sql` y lo va a ejecutar en la base de datos `poverty_analysis` automÃ¡ticamente usando docker compose.

```bash
# Levantar MySQL con Docker Compose
docker compose up -d

# Verificar que el contenedor estÃ© corriendo
docker ps
```

## ğŸ“ˆ Pipeline de Datos

### Flujo de Procesamiento

```mermaid
flowchart TD
    A[Inicio] --> B[Conectar a MySQL]
    B --> C{Â¿ConexiÃ³n Exitosa?}
    C -->|No| D[Error de ConexiÃ³n]
    C -->|SÃ­| E[Ejecutar Query SQL]
    E --> F[Extraer Datos Completos]
    F --> G[Crear Carpeta 'data']
    G --> H[Guardar Dataset Completo]
    H --> I[AnÃ¡lisis de Variables]
    I --> J[Identificar Variables NumÃ©ricas/CategÃ³ricas]
    J --> K[Crear Train/Val/Test Splits]
    K --> L[Guardar Datasets Separados]
    L --> M[Fin - Datos Listos para ML]
    
    style A fill:#e3f2fd
    style M fill:#c8e6c9
    style D fill:#ffcdd2
```

### Estructura de Datos

```mermaid
erDiagram
    fact_ocupacion {
        int id_persona PK
        int tiempo_id FK
        int ciudad_id FK
        int sector_id FK
        int condact_id FK
        float ingreso_laboral
        float ingreso_per_capita
        int horas_trabajo_semana
        boolean desea_trabajar_mas
        boolean disponible_trabajar_mas
    }
    
    dim_persona {
        int id_persona PK
        string sexo
        int edad
    }
    
    dim_tiempo {
        int tiempo_id PK
        int anio
        int mes
    }
    
    dim_ciudad {
        int ciudad_id PK
        string nombre_ciudad
    }
    
    dim_sectorlaboral {
        int sector_id PK
        string nombre_sector
    }
    
    dim_condicionactividad {
        int condact_id PK
        string nombre_condact
    }
    
    fact_ocupacion ||--|| dim_persona : "id_persona"
    fact_ocupacion ||--|| dim_tiempo : "tiempo_id"
    fact_ocupacion ||--|| dim_ciudad : "ciudad_id"
    fact_ocupacion ||--|| dim_sectorlaboral : "sector_id"
    fact_ocupacion ||--|| dim_condicionactividad : "condact_id"
```

## ğŸ”§ Uso del Pipeline

### Ejecutar Pipeline Completo

```bash
# Activar entorno virtual (si no estÃ¡ activado)
source venv/bin/activate

# Ejecutar pipeline de datos
python data_pipeline.py
```

### Salida Esperada

```
=== PIPELINE DE DATOS PARA ANÃLISIS DE POBREZA ===
Carpeta de datos: data
Extrayendo datos de MySQL...
Dataset extraÃ­do: (10000, 14)
Dataset guardado como data/poverty_dataset.csv

=== ANÃLISIS DE VARIABLES PARA PREDICCIÃ“N DE POBREZA ===
Columnas disponibles: ['id_persona', 'tiempo_id', 'anio', 'mes', 'nombre_ciudad', 'nombre_sector', 'nombre_condact', 'sexo', 'edad', 'ingreso_laboral', 'ingreso_per_capita', 'horas_trabajo_semana', 'desea_trabajar_mas', 'disponible_trabajar_mas']

Variables numÃ©ricas (8): ['id_persona', 'tiempo_id', 'anio', 'mes', 'edad', 'ingreso_laboral', 'ingreso_per_capita', 'horas_trabajo_semana']
Variables categÃ³ricas (6): ['nombre_ciudad', 'nombre_sector', 'nombre_condact', 'sexo', 'desea_trabajar_mas', 'disponible_trabajar_mas']

=== CREANDO CONJUNTOS DE DATOS ===
Train: 6400 muestras
Validation: 1600 muestras
Test: 2000 muestras

=== ARCHIVOS CREADOS ===
- data/poverty_dataset.csv (dataset completo)
- data/train_data.csv (entrenamiento)
- data/val_data.csv (validaciÃ³n)
- data/test_data.csv (test)

âœ… Pipeline completado. Datos listos para Keras!
```

## ğŸ“ Estructura de Archivos Generados

```
project/
â”œâ”€â”€ data/                          # Carpeta con datasets procesados
â”‚   â”œâ”€â”€ poverty_dataset.csv        # Dataset completo
â”‚   â”œâ”€â”€ train_data.csv            # Datos de entrenamiento (64%)
â”‚   â”œâ”€â”€ val_data.csv              # Datos de validaciÃ³n (16%)
â”‚   â””â”€â”€ test_data.csv             # Datos de test (20%)
â”œâ”€â”€ data_pipeline.py              # Script principal del pipeline
â”œâ”€â”€ bi.sql                        # Script de base de datos
â”œâ”€â”€ docker-compose.yml            # ConfiguraciÃ³n Docker
â”œâ”€â”€ requirements.txt              # Dependencias Python
â””â”€â”€ README.md                     # Este archivo
```

## ğŸ” Funcionalidades del Pipeline

### 1. ExtracciÃ³n de Datos
- **Fuente**: Base de datos MySQL con esquema dimensional
- **Query**: JOIN entre tablas fact y dimensiones
- **Campos**: 14 variables demogrÃ¡ficas y laborales

### 2. AnÃ¡lisis Exploratorio
- **IdentificaciÃ³n automÃ¡tica** de variables numÃ©ricas y categÃ³ricas
- **EstadÃ­sticas descriptivas** del dataset
- **DetecciÃ³n de indicadores de pobreza** basada en nombres de columnas

### 3. PreparaciÃ³n para ML
- **DivisiÃ³n automÃ¡tica** en train/validation/test (64%/16%/20%)
- **Manejo de variables objetivo** (target column)
- **Guardado estructurado** en formato CSV

### 4. Variables del Dataset

| Variable                  | Tipo       | DescripciÃ³n                      |
| ------------------------- | ---------- | -------------------------------- |
| `id_persona`              | NumÃ©rico   | Identificador Ãºnico de persona   |
| `anio`, `mes`             | NumÃ©rico   | Dimensiones temporales           |
| `nombre_ciudad`           | CategÃ³rico | Ciudad de residencia             |
| `nombre_sector`           | CategÃ³rico | Sector laboral                   |
| `nombre_condact`          | CategÃ³rico | CondiciÃ³n de actividad           |
| `sexo`                    | CategÃ³rico | GÃ©nero                           |
| `edad`                    | NumÃ©rico   | Edad en aÃ±os                     |
| `ingreso_laboral`         | NumÃ©rico   | Ingreso por trabajo              |
| `ingreso_per_capita`      | NumÃ©rico   | Ingreso per cÃ¡pita               |
| `horas_trabajo_semana`    | NumÃ©rico   | Horas trabajadas por semana      |
| `desea_trabajar_mas`      | Booleano   | Deseo de trabajar mÃ¡s horas      |
| `disponible_trabajar_mas` | Booleano   | Disponibilidad para trabajar mÃ¡s |

## ğŸ› ï¸ Comandos Ãštiles

### Verificar Estado de la Base de Datos
```bash
# Conectar a MySQL
mysql -u analyst -p poverty_analysis

# Verificar tablas
SHOW TABLES;

# Verificar datos
SELECT COUNT(*) FROM fact_ocupacion;
```

### Verificar Archivos Generados
```bash
# Listar archivos en carpeta data
ls -la data/

# Ver tamaÃ±o de archivos
du -h data/*.csv

# Ver primeras lÃ­neas de un archivo
head -5 data/poverty_dataset.csv
```

### Limpiar Datos Generados
```bash
# Eliminar carpeta data (cuidado: elimina todos los datasets)
rm -rf data/
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Modificar ParÃ¡metros del Pipeline

En `data_pipeline.py`, puedes ajustar:

```python
# TamaÃ±os de divisiÃ³n de datos
test_size=0.2    # 20% para test
val_size=0.2     # 20% para validaciÃ³n (del 80% restante)

# Variable objetivo personalizada
target_column = 'ingreso_per_capita'  # Cambiar variable objetivo
```

### Variables de Entorno (Opcional)

Crear archivo `.env`:
```env
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=poverty_analysis
MYSQL_USER=analyst
MYSQL_PASSWORD=analyst123
```

## ğŸ› SoluciÃ³n de Problemas

### Error de ConexiÃ³n a MySQL
```bash
# Verificar que MySQL estÃ© corriendo
sudo systemctl status mysql

# Verificar credenciales
mysql -u analyst -p
```

### Error de Dependencias
```bash
# Reinstalar dependencias
pip install --upgrade -r requirements.txt
```

### Error de Permisos
```bash
# Dar permisos de escritura a la carpeta data
chmod 755 data/
```

## ğŸ“Š PrÃ³ximos Pasos

1. **AnÃ¡lisis Exploratorio**: Crear visualizaciones con los datos generados
2. **Modelos de ML**: Implementar modelos predictivos con Keras/TensorFlow
3. **Dashboard**: Crear interfaz web para visualizaciÃ³n
4. **AutomatizaciÃ³n**: Programar ejecuciÃ³n automÃ¡tica del pipeline

# Sistema de AnÃ¡lisis de Pobreza - ML Pipeline y AplicaciÃ³n Web

## ğŸ“‹ DescripciÃ³n General

Este proyecto implementa un sistema completo de anÃ¡lisis predictivo de pobreza utilizando machine learning, con capacidades avanzadas de anÃ¡lisis de datos y una interfaz web moderna para la visualizaciÃ³n de resultados.

## ğŸš€ CaracterÃ­sticas Principales

### ğŸ”¬ AnÃ¡lisis de Datos Avanzado
- **AnÃ¡lisis Exploratorio Completo**: EvaluaciÃ³n automÃ¡tica de calidad de datos, identificaciÃ³n de patrones y estadÃ­sticas descriptivas
- **DetecciÃ³n de Indicadores de Pobreza**: IdentificaciÃ³n automÃ¡tica de variables relacionadas con pobreza
- **AnÃ¡lisis de Correlaciones**: CÃ¡lculo de importancia de caracterÃ­sticas basado en correlaciones con indicadores de pobreza
- **EvaluaciÃ³n de Calidad de Datos**: MÃ©tricas de completitud, consistencia y precisiÃ³n

### ğŸ¤– Modelos de Machine Learning
- **Redes Neuronales**: Modelos de deep learning para predicciÃ³n de alta precisiÃ³n
- **Modelos Lineales**: RegresiÃ³n logÃ­stica y lineal para anÃ¡lisis estadÃ­stico robusto
- **SelecciÃ³n AutomÃ¡tica de Modelos**: EvaluaciÃ³n automÃ¡tica y selecciÃ³n del mejor modelo
- **ValidaciÃ³n Cruzada**: EvaluaciÃ³n robusta del rendimiento de los modelos

### ğŸ“Š AplicaciÃ³n Web Interactiva
- **Interfaz Moderna**: DiseÃ±o responsive con Bootstrap 5 y Font Awesome
- **AnÃ¡lisis en Tiempo Real**: Procesamiento y visualizaciÃ³n inmediata de resultados
- **MÃºltiples Formatos de Entrada**: Soporte para Excel (.xlsx, .xls) y CSV
- **ExportaciÃ³n de Resultados**: ExportaciÃ³n a Excel, CSV y reportes completos

### ğŸ“ˆ VisualizaciÃ³n y Reportes
- **Dashboard Interactivo**: MÃ©tricas de calidad de datos, insights y recomendaciones
- **AnÃ¡lisis de Confianza**: Niveles de confianza y evaluaciÃ³n de riesgo
- **Recomendaciones AutomÃ¡ticas**: Sugerencias basadas en los resultados del anÃ¡lisis
- **Reportes Detallados**: InformaciÃ³n completa sobre predicciones y calidad del modelo

## ğŸ—ï¸ Arquitectura del Sistema

```
project/
â”œâ”€â”€ ml_pipeline/           # Pipeline de machine learning
â”‚   â”œâ”€â”€ main.py           # Punto de entrada principal
â”‚   â”œâ”€â”€ data_analyzer.py  # AnÃ¡lisis exploratorio de datos
â”‚   â”œâ”€â”€ feature_engineer.py # IngenierÃ­a de caracterÃ­sticas
â”‚   â”œâ”€â”€ model_trainer.py  # Entrenamiento de modelos
â”‚   â”œâ”€â”€ linear_models.py  # Modelos lineales
â”‚   â”œâ”€â”€ neural_network.py # Redes neuronales
â”‚   â””â”€â”€ requirements.txt  # Dependencias del pipeline
â”œâ”€â”€ prediction_app/        # AplicaciÃ³n web Flask
â”‚   â”œâ”€â”€ app.py           # Servidor Flask principal
â”‚   â”œâ”€â”€ models/          # Carga y gestiÃ³n de modelos
â”‚   â”œâ”€â”€ utils/           # Utilidades de procesamiento
â”‚   â”œâ”€â”€ static/          # Archivos estÃ¡ticos (CSS, JS)
â”‚   â”œâ”€â”€ templates/       # Plantillas HTML
â”‚   â””â”€â”€ requirements.txt # Dependencias de la app
â”œâ”€â”€ data/                # Datos de entrenamiento
â”œâ”€â”€ models/              # Modelos entrenados
â””â”€â”€ ml_results/          # Resultados del entrenamiento
```

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos
- Python 3.8+
- pip
- Git

### InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd project
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate     # Windows
```

3. **Instalar dependencias del pipeline**
```bash
cd ml_pipeline
pip install -r requirements.txt
```

4. **Instalar dependencias de la aplicaciÃ³n**
```bash
cd ../prediction_app
pip install -r requirements.txt
```

## ğŸš€ Uso del Sistema

### 1. Entrenamiento de Modelos

```bash
# Entrenar todos los modelos
python ml_pipeline/main.py --models both

# Entrenar solo modelos lineales
python ml_pipeline/main.py --models linear

# Entrenar solo redes neuronales
python ml_pipeline/main.py --models neural

# Especificar directorio de datos personalizado
python ml_pipeline/main.py --data_path data/mi_dataset.csv
```

### 2. Ejecutar la AplicaciÃ³n Web

```bash
cd prediction_app
python app.py
```

La aplicaciÃ³n estarÃ¡ disponible en: `http://localhost:5000`

### 3. Uso de la Interfaz Web

1. **Cargar Datos**: Arrastra y suelta un archivo Excel/CSV o haz clic para seleccionar
2. **Validar Archivo**: Verifica la estructura y calidad de los datos
3. **Seleccionar Modelo**: Elige entre red neuronal, logÃ­stico o lineal
4. **Realizar AnÃ¡lisis**: Ejecuta el anÃ¡lisis completo con predicciones
5. **Revisar Resultados**: Explora las mÃ©tricas, insights y recomendaciones
6. **Exportar**: Descarga los resultados en diferentes formatos

## ğŸ“Š Estructura de Datos

### Formato de Entrada Requerido

El archivo de entrada debe contener las siguientes columnas:

| Columna                 | Tipo    | DescripciÃ³n                      | Rango         |
| ----------------------- | ------- | -------------------------------- | ------------- |
| persona_key             | Entero  | ID Ãºnico de persona              | -             |
| tiempo_id               | Entero  | Identificador temporal (YYYYMM)  | 200001-203012 |
| anio                    | Entero  | AÃ±o                              | 2000-2030     |
| mes                     | Entero  | Mes                              | 1-12          |
| sector_id               | Entero  | ID del sector econÃ³mico          | 0-9           |
| condact_id              | Entero  | ID de condiciÃ³n de actividad     | 0-9           |
| sexo                    | Entero  | GÃ©nero (1=Hombre, 2=Mujer)       | 1-2           |
| ciudad_id               | Entero  | ID de la ciudad                  | -             |
| nivel_instruccion       | Entero  | Nivel de educaciÃ³n               | 0-5           |
| estado_civil            | Entero  | Estado civil                     | 0-6           |
| edad                    | Entero  | Edad en aÃ±os                     | 0-120         |
| ingreso_laboral         | Decimal | Ingreso laboral                  | â‰¥ 0           |
| ingreso_per_capita      | Decimal | Ingreso per cÃ¡pita               | â‰¥ 0           |
| horas_trabajo_semana    | Entero  | Horas trabajadas por semana      | 0-168         |
| desea_trabajar_mas      | Entero  | Deseo de trabajar mÃ¡s            | 0-4           |
| disponible_trabajar_mas | Entero  | Disponibilidad para trabajar mÃ¡s | 0-1           |

### Formato de Salida

Los resultados incluyen:

- **PredicciÃ³n de Pobreza**: 0 (No pobre) o 1 (Pobre)
- **Probabilidad**: Valor entre 0 y 1
- **Confianza**: Nivel de confianza de la predicciÃ³n
- **Nivel de Riesgo**: ClasificaciÃ³n (Bajo, Moderado, Alto, Muy Alto)
- **MÃ©tricas de Calidad**: EvaluaciÃ³n de la calidad de los datos y predicciones

## ğŸ”§ ConfiguraciÃ³n Avanzada

### ParÃ¡metros del Pipeline

```bash
python ml_pipeline/main.py --help
```

Opciones disponibles:
- `--data_path`: Ruta al archivo de datos
- `--output_dir`: Directorio de salida para resultados
- `--models`: Tipos de modelos a entrenar (linear/neural/both)
- `--target_method`: MÃ©todo para crear variable objetivo
- `--models_dir`: Directorio para modelos centralizados

### ConfiguraciÃ³n de la AplicaciÃ³n

Variables de entorno disponibles:
- `FLASK_ENV`: Entorno de Flask (development/production)
- `UPLOAD_FOLDER`: Directorio de archivos temporales
- `MAX_FILE_SIZE`: TamaÃ±o mÃ¡ximo de archivo (bytes)

## ğŸ“ˆ MÃ©tricas y EvaluaciÃ³n

### MÃ©tricas de Modelo
- **PrecisiÃ³n**: Exactitud general de las predicciones
- **Recall**: Sensibilidad para detectar casos de pobreza
- **F1-Score**: Media armÃ³nica de precisiÃ³n y recall
- **AUC-ROC**: Ãrea bajo la curva ROC

### MÃ©tricas de Calidad de Datos
- **Completitud**: Porcentaje de datos no faltantes
- **Consistencia**: VerificaciÃ³n de coherencia lÃ³gica
- **PrecisiÃ³n**: DetecciÃ³n de valores atÃ­picos
- **PuntuaciÃ³n General**: CombinaciÃ³n ponderada de todas las mÃ©tricas

## ğŸ›¡ï¸ CaracterÃ­sticas de Seguridad

- **ValidaciÃ³n de Archivos**: VerificaciÃ³n de tipo, tamaÃ±o y estructura
- **SanitizaciÃ³n de Datos**: Limpieza automÃ¡tica de datos de entrada
- **Manejo de Errores**: GestiÃ³n robusta de excepciones
- **Archivos Temporales**: Limpieza automÃ¡tica de archivos temporales

## ğŸ”„ Mantenimiento y Monitoreo

### Logs y Monitoreo
- Logs detallados de entrenamiento y predicciÃ³n
- MÃ©tricas de rendimiento del sistema
- Monitoreo de salud de la aplicaciÃ³n

### ActualizaciÃ³n de Modelos
```bash
# Reentrenar modelos con nuevos datos
python ml_pipeline/main.py --data_path data/nuevos_datos.csv

# Los modelos se actualizan automÃ¡ticamente en la aplicaciÃ³n
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ“ Soporte

Para soporte tÃ©cnico o preguntas:
- Crear un issue en el repositorio
- Contactar al equipo de desarrollo
- Revisar la documentaciÃ³n tÃ©cnica en `/docs`

## ğŸ”® Roadmap

### PrÃ³ximas CaracterÃ­sticas
- [ ] AnÃ¡lisis de series temporales
- [ ] Modelos de ensemble avanzados
- [ ] API REST para integraciÃ³n externa
- [ ] Dashboard de monitoreo en tiempo real
- [ ] AnÃ¡lisis geogrÃ¡fico y espacial
- [ ] IntegraciÃ³n con bases de datos externas

### Mejoras Planificadas
- [ ] OptimizaciÃ³n de rendimiento
- [ ] Interfaz mÃ³vil responsive
- [ ] Reportes automatizados por email
- [ ] IntegraciÃ³n con sistemas de BI
- [ ] AnÃ¡lisis de causalidad
- [ ] Modelos interpretables (SHAP, LIME)

---

**Desarrollado con â¤ï¸ para el anÃ¡lisis de pobreza y desarrollo social**
