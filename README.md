# üìä Sistema de An√°lisis de Pobreza - BI Project

## üéØ Descripci√≥n del Proyecto

Este proyecto implementa un sistema completo de Business Intelligence (BI) para el an√°lisis de pobreza, incluyendo:

- **Base de Datos MySQL**: Almacenamiento estructurado de datos demogr√°ficos y laborales
- **Pipeline de Datos**: Extracci√≥n, transformaci√≥n y preparaci√≥n de datos para ML
- **An√°lisis Predictivo**: Preparaci√≥n de datasets para modelos de machine learning

## üèóÔ∏è Arquitectura del Sistema

```mermaid
graph TB
    A[MySQL Database] --> B[Data Pipeline]
    B --> C[CSV Files]
    C --> D[ML Ready Datasets]
    
    subgraph "MySQL Database"
        A1[fact_ocupacion]
        A2[dim_persona]
        A3[dim_tiempo]
        A4[dim_ciudad]
        A5[dim_sectorlaboral]
        A6[dim_condicionactividad]
    end
    
    subgraph "Data Pipeline"
        B1[Extract Data]
        B2[Transform Data]
        B3[Split Datasets]
        B4[Save to CSV]
    end
    
    subgraph "Output Files"
        C1[poverty_dataset.csv]
        C2[train_data.csv]
        C3[val_data.csv]
        C4[test_data.csv]
    end
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
```

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.8+
- MySQL Server
- Docker (opcional, para base de datos)

### 1. Clonar el Repositorio

```bash
git clone <repository-url>
cd bi/project
```

### 2. Configurar Entorno Virtual

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En Linux/Mac:
source venv/bin/activate
# En Windows:
venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Base de Datos MySQL

Este paso va a tomar el archivo `bi.sql` y lo va a ejecutar en la base de datos `poverty_analysis` autom√°ticamente usando docker compose.

```bash
# Levantar MySQL con Docker Compose
docker compose up -d

# Verificar que el contenedor est√© corriendo
docker ps
```

## üìà Pipeline de Datos

### Flujo de Procesamiento

```mermaid
flowchart TD
    A[Inicio] --> B[Conectar a MySQL]
    B --> C{¬øConexi√≥n Exitosa?}
    C -->|No| D[Error de Conexi√≥n]
    C -->|S√≠| E[Ejecutar Query SQL]
    E --> F[Extraer Datos Completos]
    F --> G[Crear Carpeta 'data']
    G --> H[Guardar Dataset Completo]
    H --> I[An√°lisis de Variables]
    I --> J[Identificar Variables Num√©ricas/Categ√≥ricas]
    J --> K[Crear Train/Val/Test Splits]
    K --> L[Guardar Datasets Separados]
    L --> M[Fin - Datos Listos para ML]
    
    style A fill:#e3f2fd
    style M fill:#c8e6c9
    style D fill:#ffcdd2
```

### Estructura de Datos

```mermaid
erDiagram
    fact_ocupacion {
        int id_persona PK
        int tiempo_id FK
        int ciudad_id FK
        int sector_id FK
        int condact_id FK
        float ingreso_laboral
        float ingreso_per_capita
        int horas_trabajo_semana
        boolean desea_trabajar_mas
        boolean disponible_trabajar_mas
    }
    
    dim_persona {
        int id_persona PK
        string sexo
        int edad
    }
    
    dim_tiempo {
        int tiempo_id PK
        int anio
        int mes
    }
    
    dim_ciudad {
        int ciudad_id PK
        string nombre_ciudad
    }
    
    dim_sectorlaboral {
        int sector_id PK
        string nombre_sector
    }
    
    dim_condicionactividad {
        int condact_id PK
        string nombre_condact
    }
    
    fact_ocupacion ||--|| dim_persona : "id_persona"
    fact_ocupacion ||--|| dim_tiempo : "tiempo_id"
    fact_ocupacion ||--|| dim_ciudad : "ciudad_id"
    fact_ocupacion ||--|| dim_sectorlaboral : "sector_id"
    fact_ocupacion ||--|| dim_condicionactividad : "condact_id"
```

## üîß Uso del Pipeline

### Ejecutar Pipeline Completo

```bash
# Activar entorno virtual (si no est√° activado)
source venv/bin/activate

# Ejecutar pipeline de datos
python data_pipeline.py
```

### Salida Esperada

```
=== PIPELINE DE DATOS PARA AN√ÅLISIS DE POBREZA ===
Carpeta de datos: data
Extrayendo datos de MySQL...
Dataset extra√≠do: (10000, 14)
Dataset guardado como data/poverty_dataset.csv

=== AN√ÅLISIS DE VARIABLES PARA PREDICCI√ìN DE POBREZA ===
Columnas disponibles: ['id_persona', 'tiempo_id', 'anio', 'mes', 'nombre_ciudad', 'nombre_sector', 'nombre_condact', 'sexo', 'edad', 'ingreso_laboral', 'ingreso_per_capita', 'horas_trabajo_semana', 'desea_trabajar_mas', 'disponible_trabajar_mas']

Variables num√©ricas (8): ['id_persona', 'tiempo_id', 'anio', 'mes', 'edad', 'ingreso_laboral', 'ingreso_per_capita', 'horas_trabajo_semana']
Variables categ√≥ricas (6): ['nombre_ciudad', 'nombre_sector', 'nombre_condact', 'sexo', 'desea_trabajar_mas', 'disponible_trabajar_mas']

=== CREANDO CONJUNTOS DE DATOS ===
Train: 6400 muestras
Validation: 1600 muestras
Test: 2000 muestras

=== ARCHIVOS CREADOS ===
- data/poverty_dataset.csv (dataset completo)
- data/train_data.csv (entrenamiento)
- data/val_data.csv (validaci√≥n)
- data/test_data.csv (test)

‚úÖ Pipeline completado. Datos listos para Keras!
```

## üìÅ Estructura de Archivos Generados

```
project/
‚îú‚îÄ‚îÄ data/                          # Carpeta con datasets procesados
‚îÇ   ‚îú‚îÄ‚îÄ poverty_dataset.csv        # Dataset completo
‚îÇ   ‚îú‚îÄ‚îÄ train_data.csv            # Datos de entrenamiento (64%)
‚îÇ   ‚îú‚îÄ‚îÄ val_data.csv              # Datos de validaci√≥n (16%)
‚îÇ   ‚îî‚îÄ‚îÄ test_data.csv             # Datos de test (20%)
‚îú‚îÄ‚îÄ data_pipeline.py              # Script principal del pipeline
‚îú‚îÄ‚îÄ bi.sql                        # Script de base de datos
‚îú‚îÄ‚îÄ docker-compose.yml            # Configuraci√≥n Docker
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias Python
‚îî‚îÄ‚îÄ README.md                     # Este archivo
```

## üîç Funcionalidades del Pipeline

### 1. Extracci√≥n de Datos
- **Fuente**: Base de datos MySQL con esquema dimensional
- **Query**: JOIN entre tablas fact y dimensiones
- **Campos**: 14 variables demogr√°ficas y laborales

### 2. An√°lisis Exploratorio
- **Identificaci√≥n autom√°tica** de variables num√©ricas y categ√≥ricas
- **Estad√≠sticas descriptivas** del dataset
- **Detecci√≥n de indicadores de pobreza** basada en nombres de columnas

### 3. Preparaci√≥n para ML
- **Divisi√≥n autom√°tica** en train/validation/test (64%/16%/20%)
- **Manejo de variables objetivo** (target column)
- **Guardado estructurado** en formato CSV

### 4. Variables del Dataset

| Variable                  | Tipo       | Descripci√≥n                      |
| ------------------------- | ---------- | -------------------------------- |
| `id_persona`              | Num√©rico   | Identificador √∫nico de persona   |
| `anio`, `mes`             | Num√©rico   | Dimensiones temporales           |
| `nombre_ciudad`           | Categ√≥rico | Ciudad de residencia             |
| `nombre_sector`           | Categ√≥rico | Sector laboral                   |
| `nombre_condact`          | Categ√≥rico | Condici√≥n de actividad           |
| `sexo`                    | Categ√≥rico | G√©nero                           |
| `edad`                    | Num√©rico   | Edad en a√±os                     |
| `ingreso_laboral`         | Num√©rico   | Ingreso por trabajo              |
| `ingreso_per_capita`      | Num√©rico   | Ingreso per c√°pita               |
| `horas_trabajo_semana`    | Num√©rico   | Horas trabajadas por semana      |
| `desea_trabajar_mas`      | Booleano   | Deseo de trabajar m√°s horas      |
| `disponible_trabajar_mas` | Booleano   | Disponibilidad para trabajar m√°s |

## üõ†Ô∏è Comandos √ötiles

### Verificar Estado de la Base de Datos
```bash
# Conectar a MySQL
mysql -u analyst -p poverty_analysis

# Verificar tablas
SHOW TABLES;

# Verificar datos
SELECT COUNT(*) FROM fact_ocupacion;
```

### Verificar Archivos Generados
```bash
# Listar archivos en carpeta data
ls -la data/

# Ver tama√±o de archivos
du -h data/*.csv

# Ver primeras l√≠neas de un archivo
head -5 data/poverty_dataset.csv
```

### Limpiar Datos Generados
```bash
# Eliminar carpeta data (cuidado: elimina todos los datasets)
rm -rf data/
```

## üîß Configuraci√≥n Avanzada

### Modificar Par√°metros del Pipeline

En `data_pipeline.py`, puedes ajustar:

```python
# Tama√±os de divisi√≥n de datos
test_size=0.2    # 20% para test
val_size=0.2     # 20% para validaci√≥n (del 80% restante)

# Variable objetivo personalizada
target_column = 'ingreso_per_capita'  # Cambiar variable objetivo
```

### Variables de Entorno (Opcional)

Crear archivo `.env`:
```env
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=poverty_analysis
MYSQL_USER=analyst
MYSQL_PASSWORD=analyst123
```

## üêõ Soluci√≥n de Problemas

### Error de Conexi√≥n a MySQL
```bash
# Verificar que MySQL est√© corriendo
sudo systemctl status mysql

# Verificar credenciales
mysql -u analyst -p
```

### Error de Dependencias
```bash
# Reinstalar dependencias
pip install --upgrade -r requirements.txt
```

### Error de Permisos
```bash
# Dar permisos de escritura a la carpeta data
chmod 755 data/
```

## üìä Pr√≥ximos Pasos

1. **An√°lisis Exploratorio**: Crear visualizaciones con los datos generados
2. **Modelos de ML**: Implementar modelos predictivos con Keras/TensorFlow
3. **Dashboard**: Crear interfaz web para visualizaci√≥n
4. **Automatizaci√≥n**: Programar ejecuci√≥n autom√°tica del pipeline

# Sistema de An√°lisis de Pobreza - ML Pipeline y Aplicaci√≥n Web

## üìã Descripci√≥n General

Este proyecto implementa un sistema completo de an√°lisis predictivo de pobreza utilizando machine learning, con capacidades avanzadas de an√°lisis de datos y una interfaz web moderna para la visualizaci√≥n de resultados.

## üöÄ Caracter√≠sticas Principales

### üî¨ An√°lisis de Datos Avanzado
- **An√°lisis Exploratorio Completo**: Evaluaci√≥n autom√°tica de calidad de datos, identificaci√≥n de patrones y estad√≠sticas descriptivas
- **Detecci√≥n de Indicadores de Pobreza**: Identificaci√≥n autom√°tica de variables relacionadas con pobreza
- **An√°lisis de Correlaciones**: C√°lculo de importancia de caracter√≠sticas basado en correlaciones con indicadores de pobreza
- **Evaluaci√≥n de Calidad de Datos**: M√©tricas de completitud, consistencia y precisi√≥n

### ü§ñ Modelos de Machine Learning
- **Redes Neuronales**: Modelos de deep learning para predicci√≥n de alta precisi√≥n
- **Modelos Lineales**: Regresi√≥n log√≠stica y lineal para an√°lisis estad√≠stico robusto
- **Selecci√≥n Autom√°tica de Modelos**: Evaluaci√≥n autom√°tica y selecci√≥n del mejor modelo
- **Validaci√≥n Cruzada**: Evaluaci√≥n robusta del rendimiento de los modelos

### üìä Aplicaci√≥n Web Interactiva
- **Interfaz Moderna**: Dise√±o responsive con Bootstrap 5 y Font Awesome
- **An√°lisis en Tiempo Real**: Procesamiento y visualizaci√≥n inmediata de resultados
- **M√∫ltiples Formatos de Entrada**: Soporte para Excel (.xlsx, .xls) y CSV
- **Exportaci√≥n de Resultados**: Exportaci√≥n a Excel, CSV y reportes completos

### üìà Visualizaci√≥n y Reportes
- **Dashboard Interactivo**: M√©tricas de calidad de datos, insights y recomendaciones
- **An√°lisis de Confianza**: Niveles de confianza y evaluaci√≥n de riesgo
- **Recomendaciones Autom√°ticas**: Sugerencias basadas en los resultados del an√°lisis
- **Reportes Detallados**: Informaci√≥n completa sobre predicciones y calidad del modelo

## üèóÔ∏è Arquitectura del Sistema

```
project/
‚îú‚îÄ‚îÄ ml_pipeline/           # Pipeline de machine learning
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Punto de entrada principal
‚îÇ   ‚îú‚îÄ‚îÄ data_analyzer.py  # An√°lisis exploratorio de datos
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineer.py # Ingenier√≠a de caracter√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py  # Entrenamiento de modelos
‚îÇ   ‚îú‚îÄ‚îÄ linear_models.py  # Modelos lineales
‚îÇ   ‚îú‚îÄ‚îÄ neural_network.py # Redes neuronales
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt  # Dependencias del pipeline
‚îú‚îÄ‚îÄ prediction_app/        # Aplicaci√≥n web Flask
‚îÇ   ‚îú‚îÄ‚îÄ app.py           # Servidor Flask principal
‚îÇ   ‚îú‚îÄ‚îÄ models/          # Carga y gesti√≥n de modelos
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Utilidades de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ static/          # Archivos est√°ticos (CSS, JS)
‚îÇ   ‚îú‚îÄ‚îÄ templates/       # Plantillas HTML
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt # Dependencias de la app
‚îú‚îÄ‚îÄ data/                # Datos de entrenamiento
‚îú‚îÄ‚îÄ models/              # Modelos entrenados
‚îî‚îÄ‚îÄ ml_results/          # Resultados del entrenamiento
```

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### Prerrequisitos
- Python 3.8+
- pip
- Git

### Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd project
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate     # Windows
```

3. **Instalar dependencias del pipeline**
```bash
cd ml_pipeline
pip install -r requirements.txt
```

4. **Instalar dependencias de la aplicaci√≥n**
```bash
cd ../prediction_app
pip install -r requirements.txt
```

## üöÄ Uso del Sistema

### 1. Entrenamiento de Modelos

```bash
# Entrenar todos los modelos
python ml_pipeline/main.py --models both

# Entrenar solo modelos lineales
python ml_pipeline/main.py --models linear

# Entrenar solo redes neuronales
python ml_pipeline/main.py --models neural

# Especificar directorio de datos personalizado
python ml_pipeline/main.py --data_path data/mi_dataset.csv
```

### 2. Ejecutar la Aplicaci√≥n Web

```bash
cd prediction_app
python app.py
```

La aplicaci√≥n estar√° disponible en: `http://localhost:5000`

### 3. Uso de la Interfaz Web

1. **Cargar Datos**: Arrastra y suelta un archivo Excel/CSV o haz clic para seleccionar
2. **Validar Archivo**: Verifica la estructura y calidad de los datos
3. **Seleccionar Modelo**: Elige entre red neuronal, log√≠stico o lineal
4. **Realizar An√°lisis**: Ejecuta el an√°lisis completo con predicciones
5. **Revisar Resultados**: Explora las m√©tricas, insights y recomendaciones
6. **Exportar**: Descarga los resultados en diferentes formatos

## üìä Estructura de Datos

### Formato de Entrada Requerido

El archivo de entrada debe contener las siguientes columnas:

| Columna                 | Tipo    | Descripci√≥n                      | Rango         |
| ----------------------- | ------- | -------------------------------- | ------------- |
| persona_key             | Entero  | ID √∫nico de persona              | -             |
| tiempo_id               | Entero  | Identificador temporal (YYYYMM)  | 200001-203012 |
| anio                    | Entero  | A√±o                              | 2000-2030     |
| mes                     | Entero  | Mes                              | 1-12          |
| sector_id               | Entero  | ID del sector econ√≥mico          | 0-9           |
| condact_id              | Entero  | ID de condici√≥n de actividad     | 0-9           |
| sexo                    | Entero  | G√©nero (1=Hombre, 2=Mujer)       | 1-2           |
| ciudad_id               | Entero  | ID de la ciudad                  | -             |
| nivel_instruccion       | Entero  | Nivel de educaci√≥n               | 0-5           |
| estado_civil            | Entero  | Estado civil                     | 0-6           |
| edad                    | Entero  | Edad en a√±os                     | 0-120         |
| ingreso_laboral         | Decimal | Ingreso laboral                  | ‚â• 0           |
| ingreso_per_capita      | Decimal | Ingreso per c√°pita               | ‚â• 0           |
| horas_trabajo_semana    | Entero  | Horas trabajadas por semana      | 0-168         |
| desea_trabajar_mas      | Entero  | Deseo de trabajar m√°s            | 0-4           |
| disponible_trabajar_mas | Entero  | Disponibilidad para trabajar m√°s | 0-1           |

### Formato de Salida

Los resultados incluyen:

- **Predicci√≥n de Pobreza**: 0 (No pobre) o 1 (Pobre)
- **Probabilidad**: Valor entre 0 y 1
- **Confianza**: Nivel de confianza de la predicci√≥n
- **Nivel de Riesgo**: Clasificaci√≥n (Bajo, Moderado, Alto, Muy Alto)
- **M√©tricas de Calidad**: Evaluaci√≥n de la calidad de los datos y predicciones

## üîß Configuraci√≥n Avanzada

### Par√°metros del Pipeline

```bash
python ml_pipeline/main.py --help
```

Opciones disponibles:
- `--data_path`: Ruta al archivo de datos
- `--output_dir`: Directorio de salida para resultados
- `--models`: Tipos de modelos a entrenar (linear/neural/both)
- `--target_method`: M√©todo para crear variable objetivo
- `--models_dir`: Directorio para modelos centralizados

### Configuraci√≥n de la Aplicaci√≥n

Variables de entorno disponibles:
- `FLASK_ENV`: Entorno de Flask (development/production)
- `UPLOAD_FOLDER`: Directorio de archivos temporales
- `MAX_FILE_SIZE`: Tama√±o m√°ximo de archivo (bytes)

## üìà M√©tricas y Evaluaci√≥n

### M√©tricas de Modelo
- **Precisi√≥n**: Exactitud general de las predicciones
- **Recall**: Sensibilidad para detectar casos de pobreza
- **F1-Score**: Media arm√≥nica de precisi√≥n y recall
- **AUC-ROC**: √Årea bajo la curva ROC

### M√©tricas de Calidad de Datos
- **Completitud**: Porcentaje de datos no faltantes
- **Consistencia**: Verificaci√≥n de coherencia l√≥gica
- **Precisi√≥n**: Detecci√≥n de valores at√≠picos
- **Puntuaci√≥n General**: Combinaci√≥n ponderada de todas las m√©tricas

## üõ°Ô∏è Caracter√≠sticas de Seguridad

- **Validaci√≥n de Archivos**: Verificaci√≥n de tipo, tama√±o y estructura
- **Sanitizaci√≥n de Datos**: Limpieza autom√°tica de datos de entrada
- **Manejo de Errores**: Gesti√≥n robusta de excepciones
- **Archivos Temporales**: Limpieza autom√°tica de archivos temporales

## üîÑ Mantenimiento y Monitoreo

### Logs y Monitoreo
- Logs detallados de entrenamiento y predicci√≥n
- M√©tricas de rendimiento del sistema
- Monitoreo de salud de la aplicaci√≥n

### Actualizaci√≥n de Modelos
```bash
# Reentrenar modelos con nuevos datos
python ml_pipeline/main.py --data_path data/nuevos_datos.csv

# Los modelos se actualizan autom√°ticamente en la aplicaci√≥n
```

## ü§ù Contribuci√≥n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìù Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo `LICENSE` para m√°s detalles.

## üìû Soporte

Para soporte t√©cnico o preguntas:
- Crear un issue en el repositorio
- Contactar al equipo de desarrollo
- Revisar la documentaci√≥n t√©cnica en `/docs`

## üîÆ Roadmap

### Pr√≥ximas Caracter√≠sticas
- [ ] An√°lisis de series temporales
- [ ] Modelos de ensemble avanzados
- [ ] API REST para integraci√≥n externa
- [ ] Dashboard de monitoreo en tiempo real
- [ ] An√°lisis geogr√°fico y espacial
- [ ] Integraci√≥n con bases de datos externas

### Mejoras Planificadas
- [ ] Optimizaci√≥n de rendimiento
- [ ] Interfaz m√≥vil responsive
- [ ] Reportes automatizados por email
- [ ] Integraci√≥n con sistemas de BI
- [ ] An√°lisis de causalidad
- [ ] Modelos interpretables (SHAP, LIME)

---

**Desarrollado con ‚ù§Ô∏è para el an√°lisis de pobreza y desarrollo social**
