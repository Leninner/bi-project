# üìä Sistema de An√°lisis de Pobreza - BI Project

## üéØ Descripci√≥n del Proyecto

Este proyecto implementa un sistema completo de Business Intelligence (BI) para el an√°lisis de pobreza, incluyendo:

- **Base de Datos MySQL**: Almacenamiento estructurado de datos demogr√°ficos y laborales
- **Pipeline de Datos**: Extracci√≥n, transformaci√≥n y preparaci√≥n de datos para ML
- **An√°lisis Predictivo**: Preparaci√≥n de datasets para modelos de machine learning

## üèóÔ∏è Arquitectura del Sistema

```mermaid
graph TB
    A[MySQL Database] --> B[Data Pipeline]
    B --> C[CSV Files]
    C --> D[ML Ready Datasets]
    
    subgraph "MySQL Database"
        A1[fact_ocupacion]
        A2[dim_persona]
        A3[dim_tiempo]
        A4[dim_ciudad]
        A5[dim_sectorlaboral]
        A6[dim_condicionactividad]
    end
    
    subgraph "Data Pipeline"
        B1[Extract Data]
        B2[Transform Data]
        B3[Split Datasets]
        B4[Save to CSV]
    end
    
    subgraph "Output Files"
        C1[poverty_dataset.csv]
        C2[train_data.csv]
        C3[val_data.csv]
        C4[test_data.csv]
    end
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
```

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.8+
- MySQL Server
- Docker (opcional, para base de datos)

### 1. Clonar el Repositorio

```bash
git clone <repository-url>
cd bi/project
```

### 2. Configurar Entorno Virtual

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En Linux/Mac:
source venv/bin/activate
# En Windows:
venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Base de Datos MySQL

Este paso va a tomar el archivo `bi.sql` y lo va a ejecutar en la base de datos `poverty_analysis` autom√°ticamente usando docker compose.

```bash
# Levantar MySQL con Docker Compose
docker compose up -d

# Verificar que el contenedor est√© corriendo
docker ps
```

## üìà Pipeline de Datos

### Flujo de Procesamiento

```mermaid
flowchart TD
    A[Inicio] --> B[Conectar a MySQL]
    B --> C{¬øConexi√≥n Exitosa?}
    C -->|No| D[Error de Conexi√≥n]
    C -->|S√≠| E[Ejecutar Query SQL]
    E --> F[Extraer Datos Completos]
    F --> G[Crear Carpeta 'data']
    G --> H[Guardar Dataset Completo]
    H --> I[An√°lisis de Variables]
    I --> J[Identificar Variables Num√©ricas/Categ√≥ricas]
    J --> K[Crear Train/Val/Test Splits]
    K --> L[Guardar Datasets Separados]
    L --> M[Fin - Datos Listos para ML]
    
    style A fill:#e3f2fd
    style M fill:#c8e6c9
    style D fill:#ffcdd2
```

### Estructura de Datos

```mermaid
erDiagram
    fact_ocupacion {
        int id_persona PK
        int tiempo_id FK
        int ciudad_id FK
        int sector_id FK
        int condact_id FK
        float ingreso_laboral
        float ingreso_per_capita
        int horas_trabajo_semana
        boolean desea_trabajar_mas
        boolean disponible_trabajar_mas
    }
    
    dim_persona {
        int id_persona PK
        string sexo
        int edad
    }
    
    dim_tiempo {
        int tiempo_id PK
        int anio
        int mes
    }
    
    dim_ciudad {
        int ciudad_id PK
        string nombre_ciudad
    }
    
    dim_sectorlaboral {
        int sector_id PK
        string nombre_sector
    }
    
    dim_condicionactividad {
        int condact_id PK
        string nombre_condact
    }
    
    fact_ocupacion ||--|| dim_persona : "id_persona"
    fact_ocupacion ||--|| dim_tiempo : "tiempo_id"
    fact_ocupacion ||--|| dim_ciudad : "ciudad_id"
    fact_ocupacion ||--|| dim_sectorlaboral : "sector_id"
    fact_ocupacion ||--|| dim_condicionactividad : "condact_id"
```

## üîß Uso del Pipeline

### Ejecutar Pipeline Completo

```bash
# Activar entorno virtual (si no est√° activado)
source venv/bin/activate

# Ejecutar pipeline de datos
python data_pipeline.py
```

### Salida Esperada

```
=== PIPELINE DE DATOS PARA AN√ÅLISIS DE POBREZA ===
Carpeta de datos: data
Extrayendo datos de MySQL...
Dataset extra√≠do: (10000, 14)
Dataset guardado como data/poverty_dataset.csv

=== AN√ÅLISIS DE VARIABLES PARA PREDICCI√ìN DE POBREZA ===
Columnas disponibles: ['id_persona', 'tiempo_id', 'anio', 'mes', 'nombre_ciudad', 'nombre_sector', 'nombre_condact', 'sexo', 'edad', 'ingreso_laboral', 'ingreso_per_capita', 'horas_trabajo_semana', 'desea_trabajar_mas', 'disponible_trabajar_mas']

Variables num√©ricas (8): ['id_persona', 'tiempo_id', 'anio', 'mes', 'edad', 'ingreso_laboral', 'ingreso_per_capita', 'horas_trabajo_semana']
Variables categ√≥ricas (6): ['nombre_ciudad', 'nombre_sector', 'nombre_condact', 'sexo', 'desea_trabajar_mas', 'disponible_trabajar_mas']

=== CREANDO CONJUNTOS DE DATOS ===
Train: 6400 muestras
Validation: 1600 muestras
Test: 2000 muestras

=== ARCHIVOS CREADOS ===
- data/poverty_dataset.csv (dataset completo)
- data/train_data.csv (entrenamiento)
- data/val_data.csv (validaci√≥n)
- data/test_data.csv (test)

‚úÖ Pipeline completado. Datos listos para Keras!
```

## üìÅ Estructura de Archivos Generados

```
project/
‚îú‚îÄ‚îÄ data/                          # Carpeta con datasets procesados
‚îÇ   ‚îú‚îÄ‚îÄ poverty_dataset.csv        # Dataset completo
‚îÇ   ‚îú‚îÄ‚îÄ train_data.csv            # Datos de entrenamiento (64%)
‚îÇ   ‚îú‚îÄ‚îÄ val_data.csv              # Datos de validaci√≥n (16%)
‚îÇ   ‚îî‚îÄ‚îÄ test_data.csv             # Datos de test (20%)
‚îú‚îÄ‚îÄ data_pipeline.py              # Script principal del pipeline
‚îú‚îÄ‚îÄ bi.sql                        # Script de base de datos
‚îú‚îÄ‚îÄ docker-compose.yml            # Configuraci√≥n Docker
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias Python
‚îî‚îÄ‚îÄ README.md                     # Este archivo
```

## üîç Funcionalidades del Pipeline

### 1. Extracci√≥n de Datos
- **Fuente**: Base de datos MySQL con esquema dimensional
- **Query**: JOIN entre tablas fact y dimensiones
- **Campos**: 14 variables demogr√°ficas y laborales

### 2. An√°lisis Exploratorio
- **Identificaci√≥n autom√°tica** de variables num√©ricas y categ√≥ricas
- **Estad√≠sticas descriptivas** del dataset
- **Detecci√≥n de indicadores de pobreza** basada en nombres de columnas

### 3. Preparaci√≥n para ML
- **Divisi√≥n autom√°tica** en train/validation/test (64%/16%/20%)
- **Manejo de variables objetivo** (target column)
- **Guardado estructurado** en formato CSV

### 4. Variables del Dataset

| Variable                  | Tipo       | Descripci√≥n                      |
| ------------------------- | ---------- | -------------------------------- |
| `id_persona`              | Num√©rico   | Identificador √∫nico de persona   |
| `anio`, `mes`             | Num√©rico   | Dimensiones temporales           |
| `nombre_ciudad`           | Categ√≥rico | Ciudad de residencia             |
| `nombre_sector`           | Categ√≥rico | Sector laboral                   |
| `nombre_condact`          | Categ√≥rico | Condici√≥n de actividad           |
| `sexo`                    | Categ√≥rico | G√©nero                           |
| `edad`                    | Num√©rico   | Edad en a√±os                     |
| `ingreso_laboral`         | Num√©rico   | Ingreso por trabajo              |
| `ingreso_per_capita`      | Num√©rico   | Ingreso per c√°pita               |
| `horas_trabajo_semana`    | Num√©rico   | Horas trabajadas por semana      |
| `desea_trabajar_mas`      | Booleano   | Deseo de trabajar m√°s horas      |
| `disponible_trabajar_mas` | Booleano   | Disponibilidad para trabajar m√°s |

## üõ†Ô∏è Comandos √ötiles

### Verificar Estado de la Base de Datos
```bash
# Conectar a MySQL
mysql -u analyst -p poverty_analysis

# Verificar tablas
SHOW TABLES;

# Verificar datos
SELECT COUNT(*) FROM fact_ocupacion;
```

### Verificar Archivos Generados
```bash
# Listar archivos en carpeta data
ls -la data/

# Ver tama√±o de archivos
du -h data/*.csv

# Ver primeras l√≠neas de un archivo
head -5 data/poverty_dataset.csv
```

### Limpiar Datos Generados
```bash
# Eliminar carpeta data (cuidado: elimina todos los datasets)
rm -rf data/
```

## üîß Configuraci√≥n Avanzada

### Modificar Par√°metros del Pipeline

En `data_pipeline.py`, puedes ajustar:

```python
# Tama√±os de divisi√≥n de datos
test_size=0.2    # 20% para test
val_size=0.2     # 20% para validaci√≥n (del 80% restante)

# Variable objetivo personalizada
target_column = 'ingreso_per_capita'  # Cambiar variable objetivo
```

### Variables de Entorno (Opcional)

Crear archivo `.env`:
```env
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=poverty_analysis
MYSQL_USER=analyst
MYSQL_PASSWORD=analyst123
```

## üêõ Soluci√≥n de Problemas

### Error de Conexi√≥n a MySQL
```bash
# Verificar que MySQL est√© corriendo
sudo systemctl status mysql

# Verificar credenciales
mysql -u analyst -p
```

### Error de Dependencias
```bash
# Reinstalar dependencias
pip install --upgrade -r requirements.txt
```

### Error de Permisos
```bash
# Dar permisos de escritura a la carpeta data
chmod 755 data/
```

## üìä Pr√≥ximos Pasos

1. **An√°lisis Exploratorio**: Crear visualizaciones con los datos generados
2. **Modelos de ML**: Implementar modelos predictivos con Keras/TensorFlow
3. **Dashboard**: Crear interfaz web para visualizaci√≥n
4. **Automatizaci√≥n**: Programar ejecuci√≥n autom√°tica del pipeline

# Poverty Prediction ML Pipeline

Este proyecto implementa un pipeline completo de machine learning para la predicci√≥n de pobreza, con opciones flexibles para entrenar diferentes tipos de modelos.

## Caracter√≠sticas

- **An√°lisis de datos completo** con visualizaciones
- **Ingenier√≠a de caracter√≠sticas** autom√°tica
- **M√∫ltiples tipos de modelos**:
  - Modelos lineales (Regresi√≥n Lineal, Log√≠stica)
  - Redes neuronales (Clasificaci√≥n y Regresi√≥n)
- **Comparaci√≥n autom√°tica** de modelos
- **Generaci√≥n de reportes** y visualizaciones
- **Interfaz de l√≠nea de comandos** para selecci√≥n de modelos

## Instalaci√≥n

1. Clonar el repositorio
2. Crear un entorno virtual:
```bash
python -m venv venv
source venv/bin/activate  # En Linux/Mac
# o
venv\Scripts\activate  # En Windows
```

3. Instalar dependencias:
```bash
pip install -r requirements.txt
```

## Uso

### Interfaz de L√≠nea de Comandos

El pipeline se ejecuta a trav√©s del archivo `main.py` con opciones para seleccionar qu√© modelos entrenar:

#### Opciones disponibles:

- `--models linear`: Entrenar solo modelos lineales
- `--models neural`: Entrenar solo redes neuronales  
- `--models both`: Entrenar ambos tipos de modelos (por defecto)

#### Ejemplos de uso:

```bash
# Entrenar solo modelos lineales
python ml_pipeline/main.py --models linear

# Entrenar solo redes neuronales
python ml_pipeline/main.py --models neural

# Entrenar ambos tipos de modelos
python ml_pipeline/main.py --models both

# Con opciones adicionales
python ml_pipeline/main.py --models linear --data_path data/mi_dataset.csv --output_dir resultados
```

#### Argumentos completos:

```bash
python ml_pipeline/main.py --help
```

Argumentos disponibles:
- `--data_path`: Ruta al archivo CSV del dataset (por defecto: `../data/poverty_dataset.csv`)
- `--output_dir`: Directorio para guardar resultados (por defecto: `ml_results`)
- `--target_method`: M√©todo para crear la variable objetivo (por defecto: `income_threshold`)
- `--models`: Tipos de modelos a entrenar (`linear`, `neural`, `both`)

### Flujo del Pipeline

1. **An√°lisis de Datos**: Exploraci√≥n completa del dataset
2. **Ingenier√≠a de Caracter√≠sticas**: Preparaci√≥n y transformaci√≥n de datos
3. **Entrenamiento de Modelos**: Seg√∫n la selecci√≥n del usuario
4. **Comparaci√≥n de Modelos**: Evaluaci√≥n autom√°tica del rendimiento
5. **Generaci√≥n de Reportes**: Guardado de modelos, gr√°ficos y m√©tricas

### Salidas

El pipeline genera:
- **Modelos entrenados** en formato `.pkl` (lineales) y `.h5` (redes neuronales)
- **Gr√°ficos de comparaci√≥n** de rendimiento
- **Reportes de an√°lisis** de datos
- **Resumen de resultados** en formato JSON
- **Visualizaciones** de importancia de caracter√≠sticas y historial de entrenamiento

## Estructura del Proyecto

```
project/
‚îú‚îÄ‚îÄ ml_pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Punto de entrada con CLI
‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py     # Orquestador principal
‚îÇ   ‚îú‚îÄ‚îÄ data_analyzer.py     # An√°lisis de datos
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineer.py  # Ingenier√≠a de caracter√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ linear_models.py     # Modelos lineales
‚îÇ   ‚îî‚îÄ‚îÄ neural_network.py    # Redes neuronales
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ poverty_dataset.csv  # Dataset de ejemplo
‚îî‚îÄ‚îÄ ml_results/              # Resultados generados
```

## Notas

- Los modelos se comparan autom√°ticamente usando m√©tricas apropiadas (R¬≤ para regresi√≥n, accuracy para clasificaci√≥n)
- Solo se generan reportes y comparaciones si se entrenan modelos
- El pipeline es robusto y maneja errores gracefully
